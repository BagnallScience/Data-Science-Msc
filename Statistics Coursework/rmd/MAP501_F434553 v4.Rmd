---
title: "MAP501_F434553"
author: "jack Bagnall"
date: "2024-11-22"
output: html_document
---


```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

# Configure global chunk options
```

```{r}
# Install packages

# install.packages("Lahman")
# install.packages("viridis")
# install.packages("lindia")
# install.packages("corrplot")
# install.packages("knitr")
```


```{r}
# Load packages

library("tidyverse")
library("magrittr")
library("here")
library("janitor")
library("gridExtra")
library("readxl")
library("Lahman")
library("viridis")
library("lindia")
library("lme4")
library("caret")
library("pROC")
library("car")
library("corrplot")
library("knitr")
```

# Linear Regression #

```{r}
# a. Starting with the dataset Managers, create a new dataset called 'df_managers' that contains a variable win_pct, equal to the proportion of games managed resulting in a win, and the variables playerID, teamID, yearID, igID, plyrMgr.

df_managers <- Managers %>% mutate(win_pct = (W / G) * 100)

# df_managers overview

# summary(df_managers)
glimpse(df_managers)
 

```

```{r}
# Overview of Teams

glimpse(Teams)
```


```{r}

# b, i. create a new dataset called awards_man by extracting these four variables from Teams: 'yearID', 'teamID', 'DivWin', and 'CS'

df_teams <- Teams %>% select(yearID, teamID, DivWin, CS)

# Overview of df_teams

glimpse(df_teams)
summary(df_teams)

```


```{r}
# b, ii. Add together all of the variables from df_teams to the df_managers dataset

man_teams <- df_managers %>% left_join(df_teams, by = "yearID")

man_teams <- man_teams %>% 
  select(-teamID.y, -lgID)

man_teams <- man_teams %>%
  rename(
    teamID = teamID.x
  )

# man_teams <- man_teams %>% rename(teamID = teamID.y)

# Overview of man_teams

glimpse(man_teams)



```

```{r}
# b, iii. 

# Add all columns from the man_teams dataset to the df_managers dataset to the AwardsShareManagers dataset to create a new dataset to create the dataset awards_man

awards_man <- AwardsShareManagers %>% left_join(man_teams, by =  "yearID") 

glimpse(awards_man)

```

```{r}
# b, iv.

# Create new variable sqr_point_pct given by sqrt(pointsWon / pointsmax) in the awards_man dataset

awards_man <- awards_man %>% mutate(sqr_point_pct = pointsWon / pointsMax)
  
## Overview 

glimpse(awards_man)
summary(awards_man)

```

```{r}
# b, v.

# Delete the incomplete cases in the awards_man dataset and ensure all variables are treated correctly. Then drop the unused levels of the teamID variable in the awards_man dataset

# Identify incomplete cases 

incomplete_cases_awards_man <- !complete.cases(awards_man)
print(incomplete_cases_awards_man)

is.na(awards_man)


# No missing cases 

# Identify unused levels in the data (145 total levels)

table(awards_man$teamID)

levels(awards_man$teamID)

# sum(table(awards_man$teamID)) - Equals 518,188 observable variables.

# Count unused levels in teamID - Result: 114.

sum(table(awards_man$teamID) == 0)

# Drop the unused levels of teamID

awards_man$teamID <- droplevels(awards_man$teamID)

# Check if unused levels have been dropped - COMPLETE

sum(table(awards_man$teamID) == 0)

```


```{r}
# c. Use the dataset awards_man to fit a Gaussian model, spp_mod, of sqr_point_pct as a function of win_pct, DivWin, and CS. Report and interpret the results. Write out the form of the fitted model (rounded to 2 significant figures). [10 points]

# Fit a gaussian model, spp_mod or sqr_point_pct as a function of win_pct, DivWin, and CS.

spp_mod <- lm(sqr_point_pct ~ win_pct + DivWin + CS, data = awards_man)

summary(spp_mod)

```

# Report and interpret the results

A linear regression analysis was fitted to investigate the relationship between sqr_point_pct (the square root of (pointsWon / PointsMax) and three predictors: win_pct, DivWinY, and CS. The fitted model is as follows:

sqr_point_pct=0.27−0.000062⋅win_pct−0.00097⋅DivWinY+0.00039⋅CS

A linear regression model was witted to investigate the relationship between the square root of the proportion of points won by a team, divided by the total points available to win. (sqr_pointpct = PointsWon/PointsMax).  The predictors included in the model were: 

* Winning percentage (win_pct): The proportion of games won by a team during the season.

* Division Win (DivWinY): A binary variable indicating whether the team won its division (1 = Yes, 0 = No).

* Championship Series (CS): A binary variable indicating whether the team participated in the championship series (1 = Yes, 0 = No).

The regression equation was specified as: 

**sqr_point_pct=β0+β1⋅win_pct+β2⋅DivWinY+β3⋅CS+ϵ\text{sqr\_point\_pct} = \beta_0 + \beta_1 \cdot \text{win\_pct} + \beta_2 \cdot \text{DivWinY} + \beta_3 \cdot \text{CS} + \epsilon 
where β0\beta_0 is the intercept, β1\beta_1, β2\beta_2, and β3\beta_3 are the coefficients for the predictors, and ϵ\epsilon is the error term.**

Results: 


Regression Coefficients:

```{r}
# Create a table of the Regression Coefficient values

# Create a data frame of residual values

coefficients <- data.frame(
  Predictor = c("(Intercept)", "win_pct", "DivWinY", "CS"),
  Estimate = c(2.720e-01, -6.215e-05, -9.698e-04, 3.925e-04),
  `Std. Error` = c(1.900e-03, 3.084e-05, 1.032e-03, 2.464e-05),
  `t value` = c(143.175, -2.015, -0.940, 15.930),
  `Pr(>|t|)` = c("<2e-16", "0.0439", "0.3474", "<2e-16"),
  Significance = c("***", "*", "", "***")
)


# Print the Regression Coefficients data frame as a table using the knitr function

kable(coefficients, format = "html", caption = "Regression Coefficients", align = "c")

```

Residuals:

```{r}
# Create a table of Residual values for the Linear Regression model

# Create a data frame of residual values

residuals <- data.frame(
  Statistic = c("Min", "1Q", "Median", "3Q", "Max"),
  Value = c(-0.3080, -0.2505, -0.1104, 0.2165, 0.7133)
)

# Print the Residuals data frame as a table using the knitr function

kable(residuals, format = "html", caption = "Residuals", align = "c")

```


Interpretation: 

* Intercept: The baseline proportion of the points won is approximately 0.27 when all predictors are zero. This represents teams with no wins, no division title, and no Championshipo Series participation.

* win_pct: A statistically significant (p = 0.0439) but small negative relationship exists between winning percentage and the proportion of points won, suggesting a minor decrease in sqr_point_pct as win_pct increases.

* DivWinY: Division wins do not significantly predict the proportion of points won (p = 0.3474).

* CS: Championship Series participation is a strong, significant predictor  (p < 2e-16), indicating a positive association between CS and sqr_point_pct.

Residuals:



# Model fit #

* Residual Standard Error: 0.2849

* Multiple R-squared: 0.00053

* Adjusted R-squared: 0.00052

* F-statistic: 87.36 (p < 2.2e-16)

```{r}
# Create a table of the Model Summary 

# Create a data frame of the Model Summary

model_summary <- data.frame(
  Statistic = c("Residual standard error", "Degrees of freedom", "Multiple R-squared", "Adjusted R-squared", "F-statistic", "p-value"),
  Value = c("0.2849", "494936", "0.0005292", "0.0005232", "87.36", "< 2.2e-16")
)

# Print the Model Summary data frame as a table using the knitr function

kable(model_summary, format = "html", caption = "Model Summary", align = "c")
```

Interpretation:

The model explains only 0.053% of the variance in sqr_point_pct, suggesting that the included predictors do not account for much variability in the proportion of points won. While the F-statistic indicates the model is statistically significant overall, its predictive power is limited.


Conclusion

This analysis demonstrates that Championship Series participation (CS) is the strongest predictor of the proportion of points won (sqr_point_pct), with a significant positive effect. Winning percentage (win_pct) has a small but significant negative effect, while division wins (DivWinY) are not a meaningful predictor. The low R-squared value indicates that additional predictors or alternative modeling approaches may be required to better explain the variation of points won.


# 1.d State and evaluate the assumptions of the Fitted Model

Assumptions of the Fitted Model:

1. Linearity:

The relationship between each predictor and response variable (sqr_points_pct) is assumed to be linear. 

2. Homoscedasticity:  

The variance of the residuals is assumed to be constant across all levels of the predictors.

3. Independence of Errors:

The residuals (differences between observed and predicted values) are assumed to be independent. This means there should be no systematic pattern in the errors.

3. Homoscedasticity: 

The variance of the residuals is assumed to be constant across all levels of the predictors.

4. Normality of Errors: 

The residuals are assumed to follow a normal distribution, especially important for hypothesis testing on coefficients.

5. No Multicollinearity:

Predictors are assumed to not be highly correlated with each other, as multicollinearity can distort coefficient estimates and inflate standard errors.


Evaluation of assumptions

1. linearity:

Diagnostic: Residual vs. Fitted plot. If the residuals snow no clear pattern, the linearity assumption is likely satisfied.

Evaluation: If the residual plot is random, this assumption holds; otherwise, a transformation or a different model may be needed.

```{r}
# Create a Residuals vs. Fitted Plot to check if the relationship between predictors and the response variable is linear

ggplot(data = data.frame(fitted = fitted(spp_mod), 
                         residuals = residuals(spp_mod)), 
       aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.4, color = "blue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed", lwd = 0.8) +
  labs(title = "Residuals vs. Fitted: Checking Linearity", 
       x = "Fitted Values", 
       y = "Residuals") +
  theme_minimal()
```

Interpretation: The residual plot shows horizontal streaks spanning most of the vizualition, indicating that the residuals do not show a clear linear trend. The streak suggest that the linearity assumption is not fully satisfied but is not drastically violated. 

This pattern suggests that while a linear relationship is present, there may be minor model misspecifications or interactions between predictors that are not captured.


Potential solutions include adding interaction terms or transformation for predictors. Additionally, check for potential omitted variables that could explain some of the variation.


2. Homoscedasticity:

Diagnostic: Scale-Location plot or Breusch-Pagan test. A horizontal band in the Scale-Location plot suggests constant variance.

Evaluation: If residuals spread out unevenly, this assumption is violated. Remedies include weighted regression or transformations.

```{r}
# Create a Scale-Location Plot to check if residuals have constant variance (homoscedasticity) across fitted values

ggplot(data = data.frame(fitted = fitted(spp_mod), 
                         sqrt_abs_resid = sqrt(abs(residuals(spp_mod)))), 
       aes(x = fitted, y = sqrt_abs_resid)) +
  geom_point(alpha = 0.4, color = "blue") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Scale-Location Plot: Checking Homoscedasticity", 
       x = "Fitted Values", 
       y = "Sqrt(|Residuals|)") +
  theme_minimal()

```

Interpretation: The lack of clear patterns in the scale-location plot suggests missing the predictors or a need for model re-specification. The horizontal streaks are slightly less uniform compared to the linearity plot and include some curved lines a tthe bottom, covering most of the visualization.

These factors suggest minor heteroscedasticity may be present, but the overall structure suggests the model does not have sever issues with unequal variance.

A potential solution could be to test for heteroscedasticity using the Breusch-Pagan test.





3. Independence of Errors:

Diagnostic: Durbin-Watson test or visual inspection of residuals over time.

Evaluation: Independence is violated if there are systematic patterns (e.g., trends or clustering).

```{r}
# Create a Residuals vs. Observation Order Plot to check if residuals are independent, especially relevant for time-series or sequential data. Use the appropriate visual controls.

ggplot(data = data.frame(observation = seq_along(residuals(spp_mod)), 
                         residuals = residuals(spp_mod)), 
       aes(x = observation, y = residuals)) +
  geom_point(alpha = 0.4, color = "blue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed", lwd = 0.8) +
  labs(title = "Residuals vs. Observation Order: Checking Independence", 
       x = "Observation Index", 
       y = "Residuals") +
  theme_minimal() 

```

Interpretation: The residuals are scattered along the horizontal plane but are more concentrated toward the lower range of the observation index (x-axis). The scatter remains dense across the horizontal plane, consistent with the residual vs. fitted plot.

The distribution suggests possible autocorrelation or clustering in certain segments of the data.

A potential solution would be to conduct the Durbin-Watson test to check for autocorrelation. 





4. Normality of Errors:

Diagnostic: Q-Q plot of residuals or Shapiro-Wilk test. Points on the Q-Q plot should lie on a straight line if residuals are normally distributed.

Evaluation: Deviations from normality suggest potential issues with hypothesis testing but may not affect predictions if sample size is large (Central Limit Theorem).

```{r}
# Create a Q-Q Plot of Residuals to check if residuals follow a normal distribution

ggplot(data = data.frame(sample = qqnorm(residuals(spp_mod), plot = FALSE)$x, 
                         theoretical = qqnorm(residuals(spp_mod), plot = FALSE)$y), 
       aes(sample, theoretical)) +
  geom_point(alpha = 0.6, color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red", lwd = 0.8) +
  labs(title = "Q-Q Plot: Checking Normality", 
       x = "Theoretical Quantiles", 
       y = "Sample Quantiles") +
  theme_minimal()

# Create a Histogram of Residuals to visually inspect the distribution of residuals

ggplot(data = data.frame(residuals = residuals(spp_mod)), 
       aes(x = residuals)) +
  geom_histogram(binwidth = 0.05, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Residuals: Checking Normality", 
       x = "Residuals", 
       y = "Frequency") +
  theme_minimal()
```

Interpretation: The Q-Q plot shows significant deviation from the diagonal line. The plotted points form an "S" shape, with a long section following the line of best fit slightly to the right. The histogram of residuals spans from -0.3080 to 0.7133, with a frequency peak at the second bar of approximately 120,000 couints, gradually declining thereafter.

These distributions of non-normal residuals indicate the presence of outliers or skewed data.

Potential solutions include: applying transformations to the dependent variable, such as a log or Box-Cox transformation; using robust regression methods to minimize the influence of outliers; and considering non-parametric regression methods if transformations do not resolve the issue.





5. No Multicollinearity:

Diagnostic: Variance Inflation Factor (VIF). VIF values above 10 indicate high multicollinearity.

Evaluation: If multicollinearity exists, consider removing or combining correlated predictors.

```{r}
# Create a Correlation Matrix Heatmap to visualize correlations between predictors, identifying potential multicollinearity

corrplot(cor(model.matrix(~ win_pct + DivWinY + CS)), 
         method = "color", 
         tl.col = "black", 
         tl.cex = 0.8)

# Create a Variance Inflation Factor (VIF) for a numeric assessment

vif_values <- vif(spp_mod)
vif_values
```
Interpretation: Low VIF values (<5) suggest no multicollinearity issues. Multicollinearity is not a concern here, but regularization methods such as Lasso regression could be used if future predictors are added and multicollinearity increases.


Final Evaluation of the Fitted Model:

Strengths:

1. Linearity: The linearity assumption is mostly satisfied. Residual plots reveal horizontal streaks, suggesting the model captures a significant portion of the linear relationships. While minor deviations exist, they are unlikely to severely impact the model'svalidity. Adjustments, such as adding interaction terms or transformations, could further enhance model performance.

2. Homoscedasticity: Although minor heteroscedasticity is present, the overall scale-location plot does not display severe violations of the assumption. The use of robust standard errors or additional predictors could address this issue effectively.

3. Independence of errors: While some clustering of residuals was observed in the residuals vs. observation index plot, the lack of severe patterns suggests that independence of errors is not a critical issue. Any detected temporal autocorrelation could be mitigated by including lagged terms or using autoregressive models.

4. Multicollinearity: Variance Inflation Factor (VIF) values confirm no multicollinearity among predictors, making the model coefficients stable and interpretable.

5. Normality: While residuals deviate significantly from normality (as observed in the Q-Q plot), this issue can be addressed by applying robust regression methods, variable transformations (e.g., log or Box-Cox), or even non-parametric methods. Despite this, the normality assumption is often robust to minor violations in large samples.


Limitations:

1. Model fit: A lwo $R^2$ value (0.00053 indicates that the model explains very little of the variability in the response variable. This suggests potential omitted variables, inadequate predictors, or non-linear relationships that are not captured by the model)

2. Normality of Residuals: The significant deviation of residuals from normality indicates the presence of outliers or skewed data. Although this does not invalidate the model, it may affect the reliability of statistical inference (e.g., *t*-tests and *p*-values)

3. Homoscedasticity: While minor deviations exist, the presence of curved lines in the scale-location plot sugegsts a need for model re-specification or inclusion of predictors to better explain the varaince.

4. Linearity and Interaction effects: The horizontal streaks in the residuals vs. fitted plot point to possible missing interaction terms or non-linear effects, which may reduce the model's predictive power.

Conclusion:

T^he linear regression model demonstrates a reasonable fit to the data given its simplicity. While the assumptions of linearity, independence, and homoscedasticity are almost satisfied, normality violations and a low $R^2$ value suggest room for significant improvement. There are some potential solutions to address these limitations:


* Introduce transformations or interaction terms to improve linearity and account for potential non-linear effects.

* Investigate additional predictors or omitted variables that might better explain the response variable's variance.

* Address residual normality issues through robust methods or non-parametric approaches.

Overall, despite this model serving as a starting point, further refinements and exploratory data analysis would improve the models explanatory and predictive capabilities.




1.e Predict the expected value of sqr_point_pct when win_pct = 0.8, DivWin = Yes, and CS = 8. Comment on the result.


# Specify the model formula in the form of a regression equation

The regression equation is: 
  
  sqr_point_pct=0.27−0.000062⋅win_pct−0.00097⋅DivWin+0.00039⋅CS
  
# Plug the expected values into the regression equation

The expected values to subsitute into this equation:  win_pct = 0.8; DivWin = 1 (Yes);  and CS = 8, are integrated with this regression equation.


```{r}
# Plug expected values into the regression equation to predict the expected value of str_point_pct

win_pct <- 0.8
DivWin <- 1
CS <- 8

predicted_sqr_point_pct <- 0.27 - 0.000062 * win_pct - 0.00097 * DivWin + 0.00039 * CS

# Print the expected values

predicted_sqr_point_pct


```

Results:

The predicted sqr_point_pct is approximately 0.2721. My interpretation of this suggests that for a team that is winning 80% of the time, a division win, and 8 Championship Series appearances, the square root of the proportion of points won is expected to be 0.2721. The small contribution of each predictor highlights the limited variability captured by this model, consistent with the low $R^2$. This reflects the need for addtional predictors or a non-linear modeling approach.





1.f Construct 95% confidence intervals around each parameter estimate for spp_mod. Comment on the results


The 95% confidence interval for a parameter is calculated as: 


```{r}
# Construct a 95% confidence interval using confint

confint(spp_mod)
```
Results:

* Intercept: The confidence interval for the intercept does not include 0, indicating that the baseline value of sqr-point_pct (when all predictors are 0) is significantly different from 0. This confirms the reliability of the intercept estimate.

* win_pct: The confidence interval is very narrow and entirely negative, confirming a small but statistically significant negative relationship between win_pct and sqr_point_pct. This aligns with the earlier interpretation of a slight decrease in sqr_point_pct as win_pct increases.

* DivWinY: The confidence interval for DivWinY spans both negative and positive values, including 0. This suggests that the effect of division wins on sqr_point_pct is not statistically significant.

* CS: The confidence interval for CS lies entirely in the positive range, indicating a statistically significant positive relationship between Championship Series participation and sqr_point_pct.

Conclusion:

The confidence intervals suggest that win_pct and CS have statistically significant effects on sqr_point_pct, and DivWinY does not significantly predict the response variable, which may indicate that it is not a critical predictor in this model.

```{r}

```


# Logistic Regression # 

```{r}
# 2.a Plot the variable plyrMgr, which indicated if the manager was a player-manager or not, against year. Jitter the points in the vertical direction to facilitate visualisation. Comment on the graph.

# glimpse(df_managers)

# Create a jitter plot, using the variables of plyrMgr(x),and yearID(y)

ggplot(data = df_managers, aes(x = yearID, y = plyrMgr)) +
  geom_jitter(width = 0.2, height = 0.1, alpha = 0.6, color = "blue") +
  labs(title = "Jitter Plot: Distribution of Player/Manager Status Over Time",
       x = "Year",
       y = "Player/Manager Status (Yes = 1, No = 0)") +
  theme_minimal()
```
Interpretation: The jitter plot spreads points vertically to avoid overlap, making it easier to observe the distribution of plyrMgr (Yes or No) across different years (yearID).

The data shows periods with concentrated clusters of individuals categorized as "Yes" (player-managers) and "No" (not player-managers). The distribution of "Yes" values likely decrease over time, reflecting changes in team management practices such as the decline of dual-role player-managers.This pattern suggests that being a player-manager was more common in earlier years and became increasingly rare in modern seasons where specialized managerial responsibilites have taken precedent.


```{r}
# 2.b Fit a logistic regression model to plyrMgr as a function of year, report and interpret the results. Write out the form of the fitted model (rounded to 2 significant figures)

# glimpse(df_managers)
# str(data)
# summary (data)

# Fit a logistic regression model to plyrMgr as a function of yearID

logistic_regression_model <- glm(plyrMgr ~ yearID, data = df_managers, family = binomial)

# Summarise the model

summary(logistic_regression_model)

```
Results:

The logistic regression equation for the log-odds is: 

$$
\text{logit}(P) = 88.60 - 0.047 \cdot \text{yearID}
$$
Where: 

* Logit($P$) is the log-odds of being a player-manager:

$$
\text{logit}(P) = \ln\left(\frac{P}{1-P}\right)
$$
* $P$ is the probability that plyrMgr = 1

* yearID is the year

To convert the log-odds to a probability:

$$
P(\text{plyrMgr} = 1) = \frac{1}{1 + e^{-(88.60 - 0.047 \cdot \text{yearID})}}
$$
Interpretation of Results:

1. Intercept ($\beta_{0}$ = 88.60)

 * When year ID = 0, the log-odds of being a player-manager are 88.60
 
 * This value is theoretical because year 0 is not within the observed range. It represents the baseline log-odds at the extreme lower limit of yearID
 
 
2. Year Coefficient ($\beta_{1}$ =-0.047)

* For every one-unit increase in yearID, the log-odds of being a player-manager decrease by 0.047.

* This indicates a significant decline in the likelihood of being a player-manager as time progresses

3. Statistical Significance

* Both the intercept ($p$ < 2$e$ - 16) and the yearID coefficient ($p$ < 2$e$ - 16) are highly significant, confirming the decline in player-manager prevalence over time is strongly supported by the data


Model Fit

Deviance Metrics: 

* Null Deviance: 3442.4 (Model with only intercept)

* Residual Deviance: 2127.7 (Model with yearID as a predictor)

Akaike Information Criterion (AIC):

* AIC: 2131.7. A lower AIC indicated better model fit

The reduction in deviance demonstrates that including yearID significantly improves the model's fit compared to a null model


Conclusion

The logistic regression model shows a strong and statistically significant trend where the probability of being a player-manager decreases over time. Historically, player-managers were common, but their prevalence has declined sharply in modern baseball.


```{r}
# 2.c Check the overfitting using 80% - 20% split of training-test data and the seed 123. Plot comparative ROC curves and summarise your findings

# Set the seed to 123

set.seed(123)

# Split the data into training (80%) and testing (20%)

train_index <- createDataPartition(df_managers$plyrMgr, p = 0.8, list = FALSE)
train_data <- df_managers[train_index, ]
test_data <- df_managers[-train_index, ]

# Fit a logistic regression model on training data

logistic_regression_model <- glm(plyrMgr ~ yearID, data = train_data, family = binomial)

# Summarise the model
summary(logistic_regression_model)

# Predict probabilities for training and testing data

train_probs <- predict(logistic_regression_model, train_data, type = "response")
test_probs <- predict(logistic_regression_model, test_data, type = "response")

# generate ROC curves

train_roc <- roc(train_data$plyrMgr, train_probs)
test_roc <- roc(test_data$plyrMgr, test_probs)

# Plot comparative ROC curves
plot(train_roc, col = "blue", main = "ROC Curves: Training vs. Testing Data")
plot(test_roc, col = "red", add = TRUE)
legend("bottomright", legend = c("Training", "Testing"), col = c("blue", "red"), lwd = 2)

# Calculate AUC values
train_auc <- auc(train_roc)
test_auc <- auc(test_roc)

cat("Training AUC:", train_auc, "\n")
cat("Testing AUC:", test_auc, "\n")


```
Results 

* Training AUC: 0.9050

* Testing AUC: 0.8982

Interpretation

1. AUC Values:

Training AUC (0.9050): Indicates that the model performs very well on the training data, distinguishing between player-managers (plyrMgr = 1) and non-player-managers (plyrMgr = 0) with 90.5% accuracy.

* Testing AUC (0.8982): Similarly high, indicating the model generalizes well to unseen data, correctly distinguishing classes with 89.8% accuracy.

2. ROC Curve Comparison

* The ROC curves for both the training (blue) and testing (red) datasets are very close, showing that the model performs similarly on both datasets.

* The minimal gap between the curves suggests that the model is not overfitting, as it maintains strong performance on the testing dataset.


Summary of Findings

1. Overfitting Check:

* The close AUC values for training (0.9050) and testing (0.8982) data confirm that the model is not overfitting

* The model generalizes well to unseen data, indicating that it is robust

2. Conclusion 

* The logistic regression model is appropriate for predicting the likelihood of a manager being a player-manager as a function of yearID

* Its performance is consistent across both training and testing datasets, demonstrating its reliability for this analysis



```{r}
# 2.d Find Youden’s index for the training data and calculate confusion matrices at this cutoff for both training and testing data. Comment on the quality of the model.

# Youden's Index is calculated as:
# Youden's Index=Sensitivity+Specificity−1


# Generate the ROC curve for the training data

train_roc <- roc(train_data$plyrMgr, train_probs)

# Calculate the optimal cutoff using Youden's Index

optimal_cutoff <- coords(train_roc, "best", ret = "threshold", best.method = "youden")

# Ensure optimal_cutoff is numeric

optimal_cutoff <- as.numeric(optimal_cutoff)

# Display the optimal cutoff

cat("Optimal Cutoff (Youden's Index):", optimal_cutoff, "\n")


```

```{r}
# Calculate confusion matrices

# Use the optimal cutoff to classify predictions
train_predictions <- ifelse(train_probs >= optimal_cutoff, 1, 0)
test_predictions <- ifelse(test_probs >= optimal_cutoff, 1, 0)

# Create confusion matrices
train_conf_matrix <- table(Predicted = train_predictions, Actual = train_data$plyrMgr)
test_conf_matrix <- table(Predicted = test_predictions, Actual = test_data$plyrMgr)

# Print confusion matrices
cat("Confusion Matrix - Training Data:\n")
print(train_conf_matrix)

cat("Confusion Matrix - Testing Data:\n")
print(test_conf_matrix)
```

 Results and Interpretation
 
Youden's Index:

Optimal Cutoff: 0.1071893

This cutoff maximizes the difference between true positive rate (sensitivity) and false positive rate (1-specificity), providing the best balance for classification.

Confusion Matrices:

Training Data:

* True Negatives (TN):1876 Correctly predicted "N" (non-player managers)

* False Negatives (FN): 24 "Y" (player-managers) misclassified as "N"

* False Positives (FP): 608 "N" misclassified as "Y"

* True Positives (TP): 492 Correctly predicted "Y"

Observations:

* High true negative rate but a noticeable number of false positives.

* Good sensitivity (correctly identifying player-managers), though still room for improvement.

2. Testing Data:

* True Negativees (TN): 450 Correctly predicted "N.

* False Negatives (FN): 6 "Y" misclassified as "N"

* False Positives (FP): 170 "N" misclassified as "Y"

* True Positives (TP): 123 Correctly predicted "Y"

Key Observations:

* The model exhibits strong specificity, accurately identifying the majority of non-player-managers.

* Sensitivity is moderate, with a non-negligible number of false positives and false negatives.


Model Quality

1. Training Performance: 

* The model has high true negative rate, meaning it is very effective at identifying non-player-managers

* Sensitivity (true positive rate) is decent but could be better given the number of false positives and false negatives

2. Testing Performance:

* Similar trends to training data, showing the model generalizes well without overfitting.

* Testing sensitivity (ability to detect player-managers) remains strong with very few false negatives

3. Overall

* The model demonstrates good classification ability, with strong performance on both training and testing datasets

* However, the high number of false positives in both datasets suggests the model might lean towards over-predicting "Y" (player-manager), likely due to class imbalance

Model performance Summary

The logistic regression model demonstrates strong classification ability with consistent results across both training and testing datasets. The low false negative rate suggests good sensitivity, indicating that the model effectively identifies player-managers. However, the relatively high number of false positives suggests a tendency to over-predict "Y" (player-managers), likely influenced by an imbalance in the class distribution.


Recommendations for Model Improvement

Address Class Imbalance:

* Resampling Techniques: Employ oversampling methods such as SMOTE or undersampling to balance the "Y" and "N" classes.

* Weighted Classification: Apply class weights in the logistic regression model to penalize misclassification of the minority class.

Threshold Optimization:

* Further refine the decision threshold based on domain requirements, prioritizing sensitivity or specificity as needed.

Feature Engineering:

* Introduce additional predictors or interaction terms that may improve the model’s discriminative power.

Alternative Models:

* Experiment with advanced classification algorithms (e.g., Random Forest, Gradient Boosting) to assess whether they provide better classification accuracy or handle class imbalance more effectively.


```{r}
# 2.e. Using the previous results and the same cutoff in (d), calculate the sum sensitivity+specificity on the testing data as a function of lgID, i.e. the sum (sensitivity+specificity) for each lgID, and plot as a bar chart. Comment on the result. [6 points]


# Add predicted binary labels to the testing dataset using the optimal cutoff
test_data <- test_data %>%
  mutate(
    predicted = ifelse(test_probs >= 0.1071893, 1, 0),  # Replace with the cutoff from Youden's Index
    actual = ifelse(plyrMgr == "Y", 1, 0)              # Convert 'Y'/'N' to binary labels for actuals
  )

# Group testing data by lgID
test_data_by_lgID <- test_data %>% group_by(lgID)

# glimpse(test_data_by_lgID)
# head(test_data_by_lgID)
# names(test_data_by_lgID)

# Calculate sensitivity and specificity for each lgID
metrics_by_lgID <- test_data_by_lgID %>%
  summarise(
    TP = sum(predicted == 1 & actual == 1),
    FN = sum(predicted == 0 & actual == 1),
    TN = sum(predicted == 0 & actual == 0),
    FP = sum(predicted == 1 & actual == 0),
    Sensitivity = TP / (TP + FN),
    Specificity = TN / (TN + FP),
    Sum_Sens_Spec = Sensitivity + Specificity
  )

# Handle cases where TP + FN or TN + FP might be zero to avoid NaN
metrics_by_lgID <- metrics_by_lgID %>%
  mutate(
    Sensitivity = ifelse(is.nan(Sensitivity), 0, Sensitivity),
    Specificity = ifelse(is.nan(Specificity), 0, Specificity),
    Sum_Sens_Spec = Sensitivity + Specificity
  )

# Plot the sum of sensitivity and specificity for each lgID
ggplot(metrics_by_lgID, aes(x = lgID, y = Sum_Sens_Spec)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(
    title = "Sum of Sensitivity + Specificity by League (lgID)",
    x = "League (lgID)",
    y = "Sum of Sensitivity + Specificity"
  ) +
  theme_minimal()

```

The bar chart illustrates the sum of sensitivity and specificity for each league (lgId) in the testing dataset, providing insights into the model's classification performance across different leagues.

Key Observations:

1. Variation Across Leagues:

* The leagues AL (American League) and NL (National League) demonstrate the highest values for the sum of sensitivity and specificity, each nearing or ecceeding 1.5

* Conversely, AA, FL, PL, NA, and UA exhibit significantly lower values, indicating weaker model perofmance in distinguishing player-managers in these leagues

2. Top Performers:

* The high values for AL and NL suggest that the model performs well in these leagues, balancing sensitivity (true positive rate) and specificity (true negative rate) effectively. This may indicate that the characteristics or data patterns in these leagues align more closely with the predictive features used in the logistic regression model

3. Poor Performers:

* The leagues with lower values (e.g., AA, FL, PL) reflect suboptimal classification performance, with potential trade-offs between sensitivity and specificity. This could be due to smaller sample sizes, class imbalances, or data inconsistencies in these leagues, leading to reduced model accuracy.

Implications:

1. Model Generalization:

* The differences in performance suggest that the model generalizes well for some leagues (AL and NL) but struggle in others. This could indicate data heterogeneity or differing patterns across leagues that the model does not fuill capture

2. Potential Data Issues:

* Leagues with lower scores might have insufficient data or class imbalances, leading to reduced sensitivity or specificity. For instance, if a league has very few player-managers (plyrMgr = 1), the model might overpredict the majority class, reducing sensitivity

Recommendations:

1. Further Data Exploration:

* Investigate the data quality and distribution for underperforming leagues to identofy any anomalies, such as imbalanced classes or missing data

* Assess whether additional predictive features would be incorporated to enhance model performance for these leagues

2. Targeted Model Improvements:

* Apply league-specific model tuning or weights to address performance disparities

* Consider stratified sampling or league-specific thresholds to balance sensitivity and specificity

3. Broader Validation

* Perform cross-validation across all leagues to ensure the model's robustness and identify potential overfitting to dominant leagues like AL and NL


Conclusion:

The model shows strong performance in the major leagues (AL and NL), suggesting it is well suited to these contexts. However, there is room for improvement in minor or less represented leagues, where the classification results are less consistent. Addressing these disparities through targeted inteventions could enhance the model's overall reliability and applicability



 
```{r}
# 2.f. Add the variables “win_pct” to the model you created in b. Compare this and the previous model. Which model should we prefer and why?

# Fit the original model (yearID only)
logistic_regression_model <- glm(plyrMgr ~ yearID, data = df_managers, family = binomial)

# Fit the new model (yearID + win_pct)
new_logistic_regression_model <- glm(plyrMgr ~ yearID + win_pct, data = df_managers, family = binomial)

# Summarise the models
summary(logistic_regression_model)
summary(new_logistic_regression_model)

# Compare AIC values
AIC(logistic_regression_model, new_logistic_regression_model)

```
Models Overview

1. Original Model (logistic_regression_model): 
logit(P)=88.60−0.0466⋅yearID

2. New Model (new_logistic_regression_model): logit(P)=88.62−0.0466⋅yearID+0.0002⋅win_pct

Coefficients and Statistical Significance

Intercept: The intercept is significant in both models, but its practical meaning is less relevant given that year 0 is out of range.

yearID: Both models consistently show a significant decline in the likelihood of being a player-manager as time progresses.

win_pct: The effect of win_pct is not statistically significant (p < 0.05), meaning it does not add meaningful predictive power to the model


Model Metrics

Null Deviance: Original - 3442.4; New Model - 3442.4

Residual Deviance: Original - 2127.7; New Model - 2127.7

AIC: Original - 2131.7; New Model - 2133.7 

Key Observations:

1. The residual deviance is the same in both models, indicating that adding win_pct does not improve model fit

2. The new model has a slightly higher AIC (+2.0), which penalizes the additional complexity without any improvement in performance

3. The variable win_pct is not statistically significant, suggesting it does not explain additional variance in the outcome

2. Interpretation:

* Both models confirm the historical trend that the likelihood of being a player-manager declines over time

* The inclusion of win_pct was not impactful, possibly due to weak correlation between team performance and player-manager status

3. Practical implications:

* For modeling simplicity and interpretability, it is better to exclude variables that do not contribute significantly

* Future models could explore interaction terms or additional predictors that might better capture the nuances of player-manager dynamics


Conclusion

While exploring the addition of win_pct to the logistic regression model was insightful, it did not improve the model's performance. The original model is therefore preferred, as it provides the same explanatory power with fewer parameters, maintaining simplicity and interpretability.

Adding the final model equation and AIC results to the report provides clarity on the decision-making process and demonstrated that the moel selection is grounded in statistical rigor.

# Poisson Regression #



```{r}
# 3.a Create a dataset 'df_pitchers' from the dataset Pitching



```
 
 
















