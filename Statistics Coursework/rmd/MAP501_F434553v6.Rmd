---
title: "MAP501_F434553"
author: "Jack Bagnall"
date: "2024-11-22"
output: html_document
---


```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

# Configure global chunk options
```

```{r}
# Install packages

# install.packages("Lahman")
# install.packages("viridis")
# install.packages("lindia")
# install.packages("corrplot")
# install.packages("knitr")
# install.packages("lmtest")
```


```{r}
# Load packages

library("tidyverse")
library("magrittr")
library("here")
library("janitor")
library("gridExtra")
library("readxl")
library("Lahman")
library("viridis")
library("lindia")
library("lme4")
library("caret")
library("pROC")
library("car")
library("corrplot")
library("knitr")
library("lmtest")
```

# Linear Regression #

```{r}
# 1.a. Starting with the dataset Managers, create a new dataset called 'df_managers' that contains a variable win_pct, equal to the proportion of games managed resulting in a win, and the variables playerID, teamID, yearID, igID, plyrMgr.

df_managers <- Managers %>% mutate(win_pct = (W / G) * 100)

# df_managers overview

# summary(df_managers)

glimpse(df_managers)

# Overview of Teams

glimpse(Teams)
 

```
```{r}

# 1.b.i. create a new dataset called awards_man by extracting these four variables from Teams: 'yearID', 'teamID', 'DivWin', and 'CS'

df_teams <- Teams %>% select(yearID, teamID, DivWin, CS)

# Overview of df_teams

glimpse(df_teams)
summary(df_teams)

```


```{r}
# 1.b.ii. Add together all of the variables from df_teams to the df_managers dataset

man_teams <- df_managers %>% left_join(df_teams, by = "yearID")

man_teams <- man_teams %>% 
  select(-teamID.y, -lgID)

man_teams <- man_teams %>%
  rename(
    teamID = teamID.x
  )

# man_teams <- man_teams %>% rename(teamID = teamID.y)

# Overview of man_teams

glimpse(man_teams)



```

```{r}
# 1.b.iii. Add all columns from the man_teams dataset to the df_managers dataset to the AwardsShareManagers dataset to create a new dataset to create the dataset awards_man
awards_man <- AwardsShareManagers %>% left_join(man_teams, by =  "yearID") 

# glimpse(awards_man)

```

```{r}
# 1.b.iv. Create a new variable sqr_point_pct given by sqrt(pointsWon / pointsMax) in the awards_man dataset.

# Create new variable sqr_point_pct given by sqrt(pointsWon / pointsmax) in the awards_man dataset

awards_man <- awards_man %>% mutate(sqr_point_pct = pointsWon / pointsMax)
  
## Overview 

glimpse(awards_man)
summary(awards_man)

```

```{r}
# 1.b.v. Delete the incomplete cases in the awards_man dataset and ensure all variables are treated correctly. Then drop the unused levels of the teamID variable in the awards_man dataset 

# Identify incomplete cases 

incomplete_cases_awards_man <- !complete.cases(awards_man)
print(incomplete_cases_awards_man)

is.na(awards_man) # No missing cases 

# Identify unused levels in the data (145 total levels)

table(awards_man$teamID)
levels(awards_man$teamID)

# sum(table(awards_man$teamID)) - Equals 518,188 observable variables.

# Count unused levels in teamID - Result: 114.

sum(table(awards_man$teamID) == 0)

# Drop the unused levels of teamID

awards_man$teamID <- droplevels(awards_man$teamID)

# Check if unused levels have been dropped - COMPLETE

sum(table(awards_man$teamID) == 0)

```


```{r}
# 1.c. Use the dataset awards_man to fit a Gaussian model, spp_mod, of sqr_point_pct as a function of win_pct, DivWin, and CS. Report and interpret the results. Write out the form of the fitted model (rounded to 2 significant figures). [10 points]

# Fit a gaussian model, spp_mod or sqr_point_pct as a function of win_pct, DivWin, and CS.

spp_mod <- lm(sqr_point_pct ~ win_pct + DivWin + CS, data = awards_man)

# Overview
summary(spp_mod)

```

# Report and interpret the results

A linear regression analysis was fitted to investigate the relationship between sqr_point_pct (the square root of (pointsWon / PointsMax) and three predictors: win_pct, DivWinY, and CS. The fitted model is as follows:

sqr_point_pct=0.27−0.000062⋅win_pct−0.00097⋅DivWinY+0.00039⋅CS

A linear regression model was witted to investigate the relationship between the square root of the proportion of points won by a team, divided by the total points available to win. (sqr_pointpct = PointsWon/PointsMax).  The predictors included in the model were: 

* Winning percentage (win_pct): The proportion of games won by a team during the season.

* Division Win (DivWinY): A binary variable indicating whether the team won its division (1 = Yes, 0 = No).

* Championship Series (CS): A binary variable indicating whether the team participated in the championship series (1 = Yes, 0 = No).

The regression equation was specified as: 

**sqr_point_pct=β0+β1⋅win_pct+β2⋅DivWinY+β3⋅CS+ϵ\text{sqr\_point\_pct} = \beta_0 + \beta_1 \cdot \text{win\_pct} + \beta_2 \cdot \text{DivWinY} + \beta_3 \cdot \text{CS} + \epsilon 
where β0\beta_0 is the intercept, β1\beta_1, β2\beta_2, and β3\beta_3 are the coefficients for the predictors, and ϵ\epsilon is the error term.**

Results: 


Regression Coefficients:

```{r}
# Create a table of the Regression Coefficient values

# Create a data frame of residual values

coefficients <- data.frame(
  Predictor = c("(Intercept)", "win_pct", "DivWinY", "CS"),
  Estimate = c(2.720e-01, -6.215e-05, -9.698e-04, 3.925e-04),
  `Std. Error` = c(1.900e-03, 3.084e-05, 1.032e-03, 2.464e-05),
  `t value` = c(143.175, -2.015, -0.940, 15.930),
  `Pr(>|t|)` = c("<2e-16", "0.0439", "0.3474", "<2e-16"),
  Significance = c("***", "*", "", "***")
)


# Print the Regression Coefficients data frame as a table using the knitr function

kable(coefficients, format = "html", caption = "Regression Coefficients", align = "c")

```

Residuals:

```{r}
# Create a table of Residual values for the Linear Regression model

# Create a data frame of residual values

residuals <- data.frame(
  Statistic = c("Min", "1Q", "Median", "3Q", "Max"),
  Value = c(-0.3080, -0.2505, -0.1104, 0.2165, 0.7133)
)

# Print the Residuals data frame as a table using the knitr function

kable(residuals, format = "html", caption = "Residuals", align = "c")

```


Interpretation: 

* Intercept: The baseline proportion of the points won is approximately 0.27 when all predictors are zero. This represents teams with no wins, no division title, and no Championshipo Series participation.

* win_pct: A statistically significant (p = 0.0439) but small negative relationship exists between winning percentage and the proportion of points won, suggesting a minor decrease in sqr_point_pct as win_pct increases.

* DivWinY: Division wins do not significantly predict the proportion of points won (p = 0.3474).

* CS: Championship Series participation is a strong, significant predictor  (p < 2e-16), indicating a positive association between CS and sqr_point_pct.

Residuals:



# Model fit #

* Residual Standard Error: 0.2849

* Multiple R-squared: 0.00053

* Adjusted R-squared: 0.00052

* F-statistic: 87.36 (p < 2.2e-16)

```{r}
# Create a table of the Model Summary 

# Create a data frame of the Model Summary

model_summary <- data.frame(
  Statistic = c("Residual standard error", "Degrees of freedom", "Multiple R-squared", "Adjusted R-squared", "F-statistic", "p-value"),
  Value = c("0.2849", "494936", "0.0005292", "0.0005232", "87.36", "< 2.2e-16")
)

# Print the Model Summary data frame as a table using the knitr function

kable(model_summary, format = "html", caption = "Model Summary", align = "c")
```

Interpretation:

The model explains only 0.053% of the variance in sqr_point_pct, suggesting that the included predictors do not account for much variability in the proportion of points won. While the F-statistic indicates the model is statistically significant overall, its predictive power is limited.


Conclusion

This analysis demonstrates that Championship Series participation (CS) is the strongest predictor of the proportion of points won (sqr_point_pct), with a significant positive effect. Winning percentage (win_pct) has a small but significant negative effect, while division wins (DivWinY) are not a meaningful predictor. The low R-squared value indicates that additional predictors or alternative modeling approaches may be required to better explain the variation of points won.


# 1.d State and evaluate the assumptions of the Fitted Model

Assumptions of the Fitted Model:

1. Linearity:

The relationship between each predictor and response variable (sqr_points_pct) is assumed to be linear. 

2. Homoscedasticity:  

The variance of the residuals is assumed to be constant across all levels of the predictors.

3. Independence of Errors:

The residuals (differences between observed and predicted values) are assumed to be independent. This means there should be no systematic pattern in the errors.

3. Homoscedasticity: 

The variance of the residuals is assumed to be constant across all levels of the predictors.

4. Normality of Errors: 

The residuals are assumed to follow a normal distribution, especially important for hypothesis testing on coefficients.

5. No Multicollinearity:

Predictors are assumed to not be highly correlated with each other, as multicollinearity can distort coefficient estimates and inflate standard errors.


Evaluation of assumptions

1. linearity:

Diagnostic: Residual vs. Fitted plot. If the residuals snow no clear pattern, the linearity assumption is likely satisfied.

Evaluation: If the residual plot is random, this assumption holds; otherwise, a transformation or a different model may be needed.

```{r}
# Create a Residuals vs. Fitted Plot to check if the relationship between predictors and the response variable is linear

ggplot(data = data.frame(fitted = fitted(spp_mod), 
                         residuals = residuals(spp_mod)), 
       aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.4, color = "blue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed", lwd = 0.8) +
  labs(title = "Residuals vs. Fitted: Checking Linearity", 
       x = "Fitted Values", 
       y = "Residuals") +
  theme_minimal()
```

Interpretation: The residual plot shows horizontal streaks spanning most of the vizualition, indicating that the residuals do not show a clear linear trend. The streak suggest that the linearity assumption is not fully satisfied but is not drastically violated. 

This pattern suggests that while a linear relationship is present, there may be minor model misspecifications or interactions between predictors that are not captured.


Potential solutions include adding interaction terms or transformation for predictors. Additionally, check for potential omitted variables that could explain some of the variation.


2. Homoscedasticity:

Diagnostic: Scale-Location plot or Breusch-Pagan test. A horizontal band in the Scale-Location plot suggests constant variance.

Evaluation: If residuals spread out unevenly, this assumption is violated. Remedies include weighted regression or transformations.



```{r}
# Conduct a Breusch-Pagan test to check if residuals have constant variance (homoscedasticity) across fitted values

# Conduct Breusch-Pagan test

bp_test <- bptest(spp_mod)

# Print the test results

print(bp_test)

# Interpretation

if (bp_test$p.value < 0.05) {
  cat("The Breusch-Pagan test indicates heteroscedasticity (p < 0.05).\n")
} else {
  cat("The Breusch-Pagan test does not indicate heteroscedasticity (p >= 0.05).\n")
}

```

Interpretation: The extremely small p-value indicates that the null hypothesis of constant residual variance is rejected. This confirms the presence of heteroscedasticity in the residuals of the spp_mod model.

Given the detection of heteroscedasticity, the results of the model may not be reliable, as one of the key assumptions of linear regression is violated. 

To address this issue, several improvements can be made. 

1. Transform the Dependent Variable:

* Apply a transformation such as a logarithmic or square root function to the response variable. This often stabilizes variance and makes the relationship between variables more linear.

2. Weighted Least Squares (WLS):

* Assign weights inversely proportional to the variance of residuals, giving less influence to observations with high residual variance. This method adjusts the model to account for heteroscedasticity.

3.Generalized Least Squares (GLS):

* Explicitly model the structure of the residual variance, allowing for more accurate estimation of coefficients in the presence of heteroscedasticity.

4.Robust Standard Errors:

* Use heteroscedasticity-robust standard errors to adjust p-values and confidence intervals without altering the model structure. This approach is particularly useful when the main focus is on inference rather than prediction.



3. Independence of Errors:

Diagnostic: Durbin-Watson test or visual inspection of residuals over time.

Evaluation: Independence is violated if there are systematic patterns (e.g., trends or clustering).

```{r}
# Create a Residuals vs. Observation Order Plot to check if residuals are independent, especially relevant for time-series or sequential data. Use the appropriate visual controls.

ggplot(data = data.frame(observation = seq_along(residuals(spp_mod)), 
                         residuals = residuals(spp_mod)), 
       aes(x = observation, y = residuals)) +
  geom_point(alpha = 0.4, color = "blue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed", lwd = 0.8) +
  labs(title = "Residuals vs. Observation Order: Checking Independence", 
       x = "Observation Index", 
       y = "Residuals") +
  theme_minimal() 

```

Interpretation: The residuals are scattered along the horizontal plane but are more concentrated toward the lower range of the observation index (x-axis). The scatter remains dense across the horizontal plane, consistent with the residual vs. fitted plot.

The distribution suggests possible autocorrelation or clustering in certain segments of the data.

A potential solution would be to conduct the Durbin-Watson test to check for autocorrelation. 





4. Normality of Errors:

Diagnostic: Q-Q plot of residuals or Shapiro-Wilk test. Points on the Q-Q plot should lie on a straight line if residuals are normally distributed.

Evaluation: Deviations from normality suggest potential issues with hypothesis testing but may not affect predictions if sample size is large (Central Limit Theorem).

```{r}
# Create a Q-Q Plot of Residuals to check if residuals follow a normal distribution

ggplot(data = data.frame(sample = qqnorm(residuals(spp_mod), plot = FALSE)$x, 
                         theoretical = qqnorm(residuals(spp_mod), plot = FALSE)$y), 
       aes(sample, theoretical)) +
  geom_point(alpha = 0.6, color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red", lwd = 0.8) +
  labs(title = "Q-Q Plot: Checking Normality", 
       x = "Theoretical Quantiles", 
       y = "Sample Quantiles") +
  theme_minimal()

# Create a Histogram of Residuals to visually inspect the distribution of residuals

ggplot(data = data.frame(residuals = residuals(spp_mod)), 
       aes(x = residuals)) +
  geom_histogram(binwidth = 0.05, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Residuals: Checking Normality", 
       x = "Residuals", 
       y = "Frequency") +
  theme_minimal()
```

Interpretation: The Q-Q plot shows significant deviation from the diagonal line. The plotted points form an "S" shape, with a long section following the line of best fit slightly to the right. The histogram of residuals spans from -0.3080 to 0.7133, with a frequency peak at the second bar of approximately 120,000 couints, gradually declining thereafter.

These distributions of non-normal residuals indicate the presence of outliers or skewed data.

Potential solutions include: applying transformations to the dependent variable, such as a log or Box-Cox transformation; using robust regression methods to minimize the influence of outliers; and considering non-parametric regression methods if transformations do not resolve the issue.





5. No Multicollinearity:

Diagnostic: Variance Inflation Factor (VIF). VIF values above 10 indicate high multicollinearity.

Evaluation: If multicollinearity exists, consider removing or combining correlated predictors.

```{r}
# Create a Correlation Matrix Heatmap to visualize correlations between predictors, identifying potential multicollinearity

corrplot(cor(model.matrix(~ win_pct + DivWinY + CS)), 
         method = "color", 
         tl.col = "black", 
         tl.cex = 0.8)

# Create a Variance Inflation Factor (VIF) for a numeric assessment

vif_values <- vif(spp_mod)
vif_values
```
Interpretation: Low VIF values (<5) suggest no multicollinearity issues. Multicollinearity is not a concern here, but regularization methods such as Lasso regression could be used if future predictors are added and multicollinearity increases.


Final Evaluation of the Fitted Model:

Strengths:

1. Linearity:
The assumption of linearity appears largely satisfied. The residual plots show no strong evidence of non-linearity, suggesting the model captures key linear relationships. While some minor deviations are present, these are unlikely to significantly impact the model's validity. Adding interaction terms or transformations could further refine the model and improve its performance.

2. Independence of Errors:
Residual patterns in the residuals vs. observation index plot indicate no severe clustering, suggesting that independence of errors is not a major concern. Any potential temporal autocorrelation could be mitigated by incorporating lagged predictors or applying autoregressive models if necessary.

3. Multicollinearity:
Variance Inflation Factor (VIF) values confirm the absence of multicollinearity among the predictors, indicating stable and interpretable model coefficients.

4. Normality:
Although residuals show significant deviations from normality in the Q-Q plot, this issue can often be mitigated by transformations (e.g., logarithmic or Box-Cox) or using robust regression techniques. For large sample sizes, the normality assumption is typically robust to minor violations and is unlikely to substantially affect the overall validity of the model.

Limitations:

1. Homoscedasticity:
The Breusch-Pagan test results (BP = 319.21, p-value < 2.2e-16) indicate a statistically significant presence of heteroscedasticity. This suggests that the residual variance is not constant, potentially affecting the reliability of standard errors, p-values, and confidence intervals. Addressing this issue through robust standard errors, transformations, or re-specifying the model with additional predictors is necessary to enhance its reliability.

2. Model Fit:
A low $R^{2}$ value (0.00053) indicates that the model explains only a negligible portion of the variability in the response variable. This suggests the need for additional predictors, a better understanding of the underlying relationships, or consideration of non-linear modeling techniques.

3. Normality of Residuals:
The significant deviation of residuals from normality highlights the presence of outliers or skewed data. Although this does not invalidate the model, it undermines the reliability of statistical inferences (e.g., t-tests and p-values) and suggests the need for robust or non-parametric methods.

4. Linearity and Interaction Effects:
The residuals vs. fitted plot suggests some potential for missing interaction terms or non-linear effects. These could reduce the predictive accuracy and explanatory power of the model.

Conclusion:

The linear regression model demonstrates some strengths, including adequate linearity and independence of errors, as well as a lack of multicollinearity. However, the presence of heteroscedasticity, significant deviations from normality, and a very low $R^{2}$ value indicate substantial room for improvement. To address these limitations, the following steps should be considered:

* Apply transformations to the response variable or predictors to stabilize variance and improve linearity.

* Introduce interaction terms or model non-linear effects to better capture underlying relationships.

* Incorporate additional predictors or omitted variables that may explain more of the variance in the response variable.

* Use robust standard errors or non-parametric methods to account for deviations from normality and heteroscedasticity.

Overall, while the model provides a starting point for analysis, substantial modifications are necessary to enhance its explanatory and predictive capabilities.



1.e Predict the expected value of sqr_point_pct when win_pct = 0.8, DivWin = Yes, and CS = 8. Comment on the result.


# Specify the model formula in the form of a regression equation

The regression equation is: 
  
  sqr_point_pct=0.27−0.000062⋅win_pct−0.00097⋅DivWin+0.00039⋅CS
  
# Plug the expected values into the regression equation

The expected values to subsitute into this equation:  win_pct = 0.8; DivWin = 1 (Yes);  and CS = 8, are integrated with this regression equation.


```{r}
# Plug expected values into the regression equation to predict the expected value of str_point_pct

win_pct <- 0.8
DivWin <- 1
CS <- 8

predicted_sqr_point_pct <- 0.27 - 0.000062 * win_pct - 0.00097 * DivWin + 0.00039 * CS

# Print the expected values

predicted_sqr_point_pct


```

Results:

The predicted sqr_point_pct is approximately 0.2721. My interpretation of this suggests that for a team that is winning 80% of the time, a division win, and 8 Championship Series appearances, the square root of the proportion of points won is expected to be 0.2721. The small contribution of each predictor highlights the limited variability captured by this model, consistent with the low $R^2$. This reflects the need for addtional predictors or a non-linear modeling approach.





1.f Construct 95% confidence intervals around each parameter estimate for spp_mod. Comment on the results


The 95% confidence interval for a parameter is calculated as: 


```{r}
# Construct a 95% confidence interval using confint

confint(spp_mod)
```
Results:

* Intercept: The confidence interval for the intercept does not include 0, indicating that the baseline value of sqr-point_pct (when all predictors are 0) is significantly different from 0. This confirms the reliability of the intercept estimate.

* win_pct: The confidence interval is very narrow and entirely negative, confirming a small but statistically significant negative relationship between win_pct and sqr_point_pct. This aligns with the earlier interpretation of a slight decrease in sqr_point_pct as win_pct increases.

* DivWinY: The confidence interval for DivWinY spans both negative and positive values, including 0. This suggests that the effect of division wins on sqr_point_pct is not statistically significant.

* CS: The confidence interval for CS lies entirely in the positive range, indicating a statistically significant positive relationship between Championship Series participation and sqr_point_pct.

Conclusion:

The confidence intervals suggest that win_pct and CS have statistically significant effects on sqr_point_pct, and DivWinY does not significantly predict the response variable, which may indicate that it is not a critical predictor in this model.

```{r}

```


# Logistic Regression # 

```{r}
# 2.a Plot the variable plyrMgr, which indicated if the manager was a player-manager or not, against year. Jitter the points in the vertical direction to facilitate visualisation. Comment on the graph.

# glimpse(df_managers)

# Create a jitter plot, using the variables of plyrMgr(x),and yearID(y)

ggplot(data = df_managers, aes(x = yearID, y = plyrMgr)) +
  geom_jitter(width = 0.2, height = 0.1, alpha = 0.6, color = "blue") +
  labs(title = "Jitter Plot: Distribution of Player/Manager Status Over Time",
       x = "Year",
       y = "Player/Manager Status (Yes = 1, No = 0)") +
  theme_minimal()
```
Interpretation: The jitter plot spreads points vertically to avoid overlap, making it easier to observe the distribution of plyrMgr (Yes or No) across different years (yearID).

The data shows periods with concentrated clusters of individuals categorized as "Yes" (player-managers) and "No" (not player-managers). The distribution of "Yes" values likely decrease over time, reflecting changes in team management practices such as the decline of dual-role player-managers.This pattern suggests that being a player-manager was more common in earlier years and became increasingly rare in modern seasons where specialized managerial responsibilites have taken precedent.


```{r}
# 2.b Fit a logistic regression model to plyrMgr as a function of year, report and interpret the results. Write out the form of the fitted model (rounded to 2 significant figures)

# glimpse(df_managers)
# str(data)
# summary (data)

# Fit a logistic regression model to plyrMgr as a function of yearID

logistic_regression_model <- glm(plyrMgr ~ yearID, data = df_managers, family = binomial)

# Summarise the model

summary(logistic_regression_model)

```
Results:

The logistic regression equation for the log-odds is: 

$$
\text{logit}(P) = 88.60 - 0.047 \cdot \text{yearID}
$$
Where: 

* Logit($P$) is the log-odds of being a player-manager:

$$
\text{logit}(P) = \ln\left(\frac{P}{1-P}\right)
$$
* $P$ is the probability that plyrMgr = 1

* yearID is the year

To convert the log-odds to a probability:

$$
P(\text{plyrMgr} = 1) = \frac{1}{1 + e^{-(88.60 - 0.047 \cdot \text{yearID})}}
$$
Interpretation of Results:

1. Intercept ($\beta_{0}$ = 88.60)

 * When year ID = 0, the log-odds of being a player-manager are 88.60
 
 * This value is theoretical because year 0 is not within the observed range. It represents the baseline log-odds at the extreme lower limit of yearID
 
 
2. Year Coefficient ($\beta_{1}$ =-0.047)

* For every one-unit increase in yearID, the log-odds of being a player-manager decrease by 0.047.

* This indicates a significant decline in the likelihood of being a player-manager as time progresses

3. Statistical Significance

* Both the intercept ($p$ < 2$e$ - 16) and the yearID coefficient ($p$ < 2$e$ - 16) are highly significant, confirming the decline in player-manager prevalence over time is strongly supported by the data


Model Fit

Deviance Metrics: 

* Null Deviance: 3442.4 (Model with only intercept)

* Residual Deviance: 2127.7 (Model with yearID as a predictor)

Akaike Information Criterion (AIC):

* AIC: 2131.7. A lower AIC indicated better model fit

The reduction in deviance demonstrates that including yearID significantly improves the model's fit compared to a null model


Conclusion

The logistic regression model shows a strong and statistically significant trend where the probability of being a player-manager decreases over time. Historically, player-managers were common, but their prevalence has declined sharply in modern baseball.


```{r}
# 2.c Check the overfitting using 80% - 20% split of training-test data and the seed 123. Plot comparative ROC curves and summarise your findings

# Set the seed to 123

set.seed(123)

# Split the data into training (80%) and testing (20%)

train_index <- createDataPartition(df_managers$plyrMgr, p = 0.8, list = FALSE)
train_data <- df_managers[train_index, ]
test_data <- df_managers[-train_index, ]

# Fit a logistic regression model on training data

logistic_regression_model <- glm(plyrMgr ~ yearID, data = train_data, family = binomial)

# Summarise the model
summary(logistic_regression_model)

# Predict probabilities for training and testing data

train_probs <- predict(logistic_regression_model, train_data, type = "response")
test_probs <- predict(logistic_regression_model, test_data, type = "response")

# generate ROC curves

train_roc <- roc(train_data$plyrMgr, train_probs)
test_roc <- roc(test_data$plyrMgr, test_probs)

# Plot comparative ROC curves

plot(train_roc, col = "blue", main = "ROC Curves: Training vs. Testing Data")
plot(test_roc, col = "red", add = TRUE)
legend("bottomright", legend = c("Training", "Testing"), col = c("blue", "red"), lwd = 2)

# Calculate AUC values

train_auc <- auc(train_roc)
test_auc <- auc(test_roc)

cat("Training AUC:", train_auc, "\n")
cat("Testing AUC:", test_auc, "\n")


```
Results 

* Training AUC: 0.9050

* Testing AUC: 0.8982

Interpretation

1. AUC Values:

Training AUC (0.9050): Indicates that the model performs very well on the training data, distinguishing between player-managers (plyrMgr = 1) and non-player-managers (plyrMgr = 0) with 90.5% accuracy.

* Testing AUC (0.8982): Similarly high, indicating the model generalizes well to unseen data, correctly distinguishing classes with 89.8% accuracy.

2. ROC Curve Comparison

* The ROC curves for both the training (blue) and testing (red) datasets are very close, showing that the model performs similarly on both datasets.

* The minimal gap between the curves suggests that the model is not overfitting, as it maintains strong performance on the testing dataset.


Summary of Findings

1. Overfitting Check:

* The close AUC values for training (0.9050) and testing (0.8982) data confirm that the model is not overfitting

* The model generalizes well to unseen data, indicating that it is robust

2. Conclusion 

* The logistic regression model is appropriate for predicting the likelihood of a manager being a player-manager as a function of yearID

* Its performance is consistent across both training and testing datasets, demonstrating its reliability for this analysis



```{r}
# 2.d Find Youden’s index for the training data and calculate confusion matrices at this cutoff for both training and testing data. Comment on the quality of the model.

# Youden's Index is calculated as:Youden's Index=Sensitivity+Specificity−1


# Generate the ROC curve for the training data

train_roc <- roc(train_data$plyrMgr, train_probs)

# Calculate the optimal cutoff using Youden's Index

optimal_cutoff <- coords(train_roc, "best", ret = "threshold", best.method = "youden")

# Ensure optimal_cutoff is numeric

optimal_cutoff <- as.numeric(optimal_cutoff)

# Display the optimal cutoff

cat("Optimal Cutoff (Youden's Index):", optimal_cutoff, "\n")


```

```{r}
# Calculate confusion matrices

# Use the optimal cutoff to classify predictions

train_predictions <- ifelse(train_probs >= optimal_cutoff, 1, 0)
test_predictions <- ifelse(test_probs >= optimal_cutoff, 1, 0)

# Create confusion matrices

train_conf_matrix <- table(Predicted = train_predictions, Actual = train_data$plyrMgr)
test_conf_matrix <- table(Predicted = test_predictions, Actual = test_data$plyrMgr)

# Print confusion matrices

cat("Confusion Matrix - Training Data:\n")
print(train_conf_matrix)

cat("Confusion Matrix - Testing Data:\n")
print(test_conf_matrix)
```

 Results and Interpretation
 
Youden's Index:

Optimal Cutoff: 0.1071893

This cutoff maximizes the difference between true positive rate (sensitivity) and false positive rate (1-specificity), providing the best balance for classification.

Confusion Matrices:

Training Data:

* True Negatives (TN):1876 Correctly predicted "N" (non-player managers)

* False Negatives (FN): 24 "Y" (player-managers) misclassified as "N"

* False Positives (FP): 608 "N" misclassified as "Y"

* True Positives (TP): 492 Correctly predicted "Y"

Observations:

* High true negative rate but a noticeable number of false positives.

* Good sensitivity (correctly identifying player-managers), though still room for improvement.

2. Testing Data:

* True Negativees (TN): 450 Correctly predicted "N.

* False Negatives (FN): 6 "Y" misclassified as "N"

* False Positives (FP): 170 "N" misclassified as "Y"

* True Positives (TP): 123 Correctly predicted "Y"

Key Observations:

* The model exhibits strong specificity, accurately identifying the majority of non-player-managers.

* Sensitivity is moderate, with a non-negligible number of false positives and false negatives.


Model Quality

1. Training Performance: 

* The model has high true negative rate, meaning it is very effective at identifying non-player-managers

* Sensitivity (true positive rate) is decent but could be better given the number of false positives and false negatives

2. Testing Performance:

* Similar trends to training data, showing the model generalizes well without overfitting.

* Testing sensitivity (ability to detect player-managers) remains strong with very few false negatives

3. Overall

* The model demonstrates good classification ability, with strong performance on both training and testing datasets

* However, the high number of false positives in both datasets suggests the model might lean towards over-predicting "Y" (player-manager), likely due to class imbalance

Model performance Summary

The logistic regression model demonstrates strong classification ability with consistent results across both training and testing datasets. The low false negative rate suggests good sensitivity, indicating that the model effectively identifies player-managers. However, the relatively high number of false positives suggests a tendency to over-predict "Y" (player-managers), likely influenced by an imbalance in the class distribution.


Recommendations for Model Improvement

Address Class Imbalance:

* Resampling Techniques: Employ oversampling methods such as SMOTE or undersampling to balance the "Y" and "N" classes.

* Weighted Classification: Apply class weights in the logistic regression model to penalize misclassification of the minority class.

Threshold Optimization:

* Further refine the decision threshold based on domain requirements, prioritizing sensitivity or specificity as needed.

Feature Engineering:

* Introduce additional predictors or interaction terms that may improve the model’s discriminative power.

Alternative Models:

* Experiment with advanced classification algorithms (e.g., Random Forest, Gradient Boosting) to assess whether they provide better classification accuracy or handle class imbalance more effectively.


```{r}
# 2.e. Using the previous results and the same cutoff in (d), calculate the sum sensitivity+specificity on the testing data as a function of lgID, i.e. the sum (sensitivity+specificity) for each lgID, and plot as a bar chart. Comment on the result. [6 points]


# Add predicted binary labels to the testing dataset using the optimal cutoff

test_data <- test_data %>%
  mutate(
    predicted = ifelse(test_probs >= 0.1071893, 1, 0),  # Replace with the cutoff from Youden's Index
    actual = ifelse(plyrMgr == "Y", 1, 0)              # Convert 'Y'/'N' to binary labels for actuals
  )

# Group testing data by lgID

test_data_by_lgID <- test_data %>% group_by(lgID)

# glimpse(test_data_by_lgID)
# head(test_data_by_lgID)
# names(test_data_by_lgID)

# Calculate sensitivity and specificity for each lgID

metrics_by_lgID <- test_data_by_lgID %>%
  summarise(
    TP = sum(predicted == 1 & actual == 1),
    FN = sum(predicted == 0 & actual == 1),
    TN = sum(predicted == 0 & actual == 0),
    FP = sum(predicted == 1 & actual == 0),
    Sensitivity = TP / (TP + FN),
    Specificity = TN / (TN + FP),
    Sum_Sens_Spec = Sensitivity + Specificity
  )

# Handle cases where TP + FN or TN + FP might be zero to avoid NaN

metrics_by_lgID <- metrics_by_lgID %>%
  mutate(
    Sensitivity = ifelse(is.nan(Sensitivity), 0, Sensitivity),
    Specificity = ifelse(is.nan(Specificity), 0, Specificity),
    Sum_Sens_Spec = Sensitivity + Specificity
  )

# Plot the sum of sensitivity and specificity for each lgID

ggplot(metrics_by_lgID, aes(x = lgID, y = Sum_Sens_Spec)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(
    title = "Sum of Sensitivity + Specificity by League (lgID)",
    x = "League (lgID)",
    y = "Sum of Sensitivity + Specificity"
  ) +
  theme_minimal()

```

The bar chart illustrates the sum of sensitivity and specificity for each league (lgId) in the testing dataset, providing insights into the model's classification performance across different leagues.

Key Observations:

1. Variation Across Leagues:

* The leagues AL (American League) and NL (National League) demonstrate the highest values for the sum of sensitivity and specificity, each nearing or ecceeding 1.5

* Conversely, AA, FL, PL, NA, and UA exhibit significantly lower values, indicating weaker model perofmance in distinguishing player-managers in these leagues

2. Top Performers:

* The high values for AL and NL suggest that the model performs well in these leagues, balancing sensitivity (true positive rate) and specificity (true negative rate) effectively. This may indicate that the characteristics or data patterns in these leagues align more closely with the predictive features used in the logistic regression model

3. Poor Performers:

* The leagues with lower values (e.g., AA, FL, PL) reflect suboptimal classification performance, with potential trade-offs between sensitivity and specificity. This could be due to smaller sample sizes, class imbalances, or data inconsistencies in these leagues, leading to reduced model accuracy.

Implications:

1. Model Generalization:

* The differences in performance suggest that the model generalizes well for some leagues (AL and NL) but struggle in others. This could indicate data heterogeneity or differing patterns across leagues that the model does not fuill capture

2. Potential Data Issues:

* Leagues with lower scores might have insufficient data or class imbalances, leading to reduced sensitivity or specificity. For instance, if a league has very few player-managers (plyrMgr = 1), the model might overpredict the majority class, reducing sensitivity

Recommendations:

1. Further Data Exploration:

* Investigate the data quality and distribution for underperforming leagues to identofy any anomalies, such as imbalanced classes or missing data

* Assess whether additional predictive features would be incorporated to enhance model performance for these leagues

2. Targeted Model Improvements:

* Apply league-specific model tuning or weights to address performance disparities

* Consider stratified sampling or league-specific thresholds to balance sensitivity and specificity

3. Broader Validation

* Perform cross-validation across all leagues to ensure the model's robustness and identify potential overfitting to dominant leagues like AL and NL


Conclusion:

The model shows strong performance in the major leagues (AL and NL), suggesting it is well suited to these contexts. However, there is room for improvement in minor or less represented leagues, where the classification results are less consistent. Addressing these disparities through targeted inteventions could enhance the model's overall reliability and applicability



 
```{r}
# 2.f. Add the variables “win_pct” to the model you created in b. Compare this and the previous model. Which model should we prefer and why?

# Fit the original model (yearID only)

logistic_regression_model <- glm(plyrMgr ~ yearID, data = df_managers, family = binomial)

# Fit the new model (yearID + win_pct)

new_logistic_regression_model <- glm(plyrMgr ~ yearID + win_pct, data = df_managers, family = binomial)

# Summarise the models

summary(logistic_regression_model)
summary(new_logistic_regression_model)

# Compare AIC values

AIC(logistic_regression_model, new_logistic_regression_model)

```
Models Overview

1. Original Model (logistic_regression_model): 
logit(P)=88.60−0.0466⋅yearID

2. New Model (new_logistic_regression_model): logit(P)=88.62−0.0466⋅yearID+0.0002⋅win_pct

Coefficients and Statistical Significance

Intercept: The intercept is significant in both models, but its practical meaning is less relevant given that year 0 is out of range.

yearID: Both models consistently show a significant decline in the likelihood of being a player-manager as time progresses.

win_pct: The effect of win_pct is not statistically significant (p < 0.05), meaning it does not add meaningful predictive power to the model


Model Metrics

Null Deviance: Original - 3442.4; New Model - 3442.4

Residual Deviance: Original - 2127.7; New Model - 2127.7

AIC: Original - 2131.7; New Model - 2133.7 

Key Observations:

1. The residual deviance is the same in both models, indicating that adding win_pct does not improve model fit

2. The new model has a slightly higher AIC (+2.0), which penalizes the additional complexity without any improvement in performance

3. The variable win_pct is not statistically significant, suggesting it does not explain additional variance in the outcome

2. Interpretation:

* Both models confirm the historical trend that the likelihood of being a player-manager declines over time

* The inclusion of win_pct was not impactful, possibly due to weak correlation between team performance and player-manager status

3. Practical implications:

* For modeling simplicity and interpretability, it is better to exclude variables that do not contribute significantly

* Future models could explore interaction terms or additional predictors that might better capture the nuances of player-manager dynamics


Conclusion

While exploring the addition of win_pct to the logistic regression model was insightful, it did not improve the model's performance. The original model is therefore preferred, as it provides the same explanatory power with fewer parameters, maintaining simplicity and interpretability.

Adding the final model equation and AIC results to the report provides clarity on the decision-making process and demonstrated that the moel selection is grounded in statistical rigor.

# Poisson Regression #



```{r}
# 3.a Create a dataset 'df_pitchers' from the dataset Pitching which adds a variable "innings" given by IPouts/3. Add the variables "weight", "height" and "throws" from the People dataset.Also, remove incomplete cases from the dataset. 

# Filter pitchers who faced at least one batter (IPouts > 0)

df_pitchers <- Pitching %>%
  filter(IPouts > 0) %>%
  mutate(innings = IPouts / 3) %>%
  left_join(People %>% select(playerID, weight, height, throws), by = "playerID") %>%
  na.omit()

# Overview of df_pitchers

glimpse(df_pitchers)

```
 
 


```{r}
# 3.b Plot a histogram of the number of shutouts (games pitched with no runs by the opposing team). Why would a Poisson model be appropriate for such data? 

# Plot histogram

ggplot(df_pitchers, aes(x = SHO)) +
  geom_histogram(binwidth = 1, color = "black", fill = "blue") +
  labs(title = "Histogram of Shutouts", x = "Number of Shutouts", y = "Frequency")

```

The histogram of the number of shutouts shows a highly skewed distribution, with the majority of observations concentrated at zero and a rapid decline in frequency as the number of shutouts increases. This pattern indicates a count variable with non-negative integer values and a strong concentration around lower counts, which aligns well with the characteristics of a Poisson distribution.

A Poisson model is appropriate for this data because:

1. Count Data: Shutouts are count data (discrete, non-negative integers), which is the type of data that a Poisson model is designed to handle.

2. Skewness: The histogram demonstrates a strong right skew, a typical feature of Poisson-distributed data when the mean is low.

3. Non-Negativity: The Poisson distribution only generates non-negative values, which corresponds to the nature of shutouts.

4. Rare Events: Shutouts are relatively rare events in baseball, and the Poisson model is well-suited for modeling the occurrence of rare events within a fixed interval (e.g., games or seasons).

5. No Upper Limit: There is no inherent upper limit on the number of shutouts a pitcher can achieve, which aligns with the theoretical properties of the Poisson distribution.

Overall, the observed characteristics of the data suggest that a Poisson model would provide an appropriate framework for modeling shutouts, enabling the analysis of relationships between shutouts and explanatory variables while respecting the distribution's properties.


```{r}
# 3.c Plot a graph of the number of shutouts as a function of innings pitched. Colour the points by the hand the pitcher throws with. Jitter the data on the vertical axis to improve readability. Comment on the graph.

# Scatterplot with jitter

ggplot(df_pitchers, aes(x = innings, y = SHO, color = throws)) +
  geom_point(position = position_jitter(width = 0, height = 0.1)) +
  labs(title = "Shutouts vs. Innings Pitched",
       x = "Innings Pitched",
       y = "Number of Shutouts") +
  theme_minimal()

```
The scatter plot visualizes the relationship between the number of shutouts and the number of innings pitched. Each point is color-coded by the hand the pitcher throws with: left-handed (L), right-handed (R), or switch-handed (S). Jittering has been applied to the vertical axis to reduce overlapping points, enhancing readability.

Key Observations:

1. Positive Relationship: There is a general trend where pitchers who pitch more innings tend to have a higher number of shutouts. This aligns with expectations, as pitchers with more opportunities are more likely to achieve shutouts.

2. Distribution by Throws:

Right-handed (R) pitchers dominate the dataset in terms of frequency, indicated by the dense concentration of green points.
Left-handed (L) pitchers are less frequent but are distributed similarly to right-handed pitchers in terms of shutouts and innings pitched.
Switch-handed (S) pitchers are rare and appear to have minimal representation in the dataset.

3. Clustering: Most of the data points are clustered near zero shutouts and low innings pitched, reflecting the rarity of shutouts and the presence of pitchers with fewer opportunities.

4. Spread in Higher Innings: As the number of innings pitched increases, the number of shutouts also increases, but variability in the number of shutouts grows. This suggests that while innings pitched is a predictor of shutouts, other factors (e.g., skill, team performance) likely influence shutout frequency.

5. Jittering Effect: The jittering effectively separates overlapping points, particularly for low shutout counts, allowing for better visualization of the underlying data density.

Summary:

The plot confirms a logical relationship between innings pitched and shutouts, with the type of throwing hand not appearing to play a significant role in the number of shutouts achieved. However, the dominance of right-handed pitchers reflects their prevalence in the dataset rather than any performance-based conclusion.

```{r}
# 3.d . Create a multiple Poisson model, poisson_mod1, of shutouts as a function of innings, weight, height, and throws. Report and interpret the results. Find the p-value for each of the four predictors using analysis of variance. Interpret the results and mathematically explain what is meant by the p-value associated to each predictor.

# Fit Poisson regression model

poisson_mod1 <- glm(SHO ~ innings + weight + height + throws, 
                    family = poisson, data = df_pitchers)

# Model summary

summary(poisson_mod1)

# ANOVA for p-values

anova(poisson_mod1, test = "Chisq")

```
Results:

Model Coefficients

The Poisson regression model (poisson_mod1) models the number of shutouts (SHO) as a function of innings pitched, weight, height, and throwing hand (categorical variable throws). Below is the interpretation of the output:

1. Intercept

The estimated log count of shutouts when all predictors are at their reference levels is -7.070, corresponding to a very small expected number of shutouts.

* Interpretation: The base rate of shutouts for pitchers with average weight, height, no innings pitched, and the reference throwing hand is near zero.

2. Innings

The coefficient for innings is 0.02053, and it is highly significant (p < 2e-16).

* Interpretation: Each additional inning pitched increases the expected log count of shutouts by 0.02053. The multiplicative effect on the number of shutouts is exp(0.02053) ≈ 1.021. This means each inning increases shutouts by about 2.1%, holding other variables constant.

3. Weight

The coefficient for weight is -0.009469, and it is highly significant (p < 2e-16).

* Interpretation: A 1-unit (1 pound) increase in weight decreases the log count of shutouts by 0.009469. The multiplicative effect is exp(-0.009469) ≈ 0.99, corresponding to a 0.95% decrease in expected shutouts.

4. Height

The coefficient for height is 0.06254, and it is highly significant (p = 2.28e-15).

* Interpretation: A 1-unit (1 inch) increase in height increases the log count of shutouts by 0.06254. The multiplicative effect is exp(0.06254) ≈ 1.0645, corresponding to a 6.45% increase in expected shutouts.

5. Throws

The baseline level is left-handed (L), and the coefficients for the other categories are compared to this.

* throwsR: Right-handed pitchers have a lower expected log count of shutouts by 0.1022 compared to left-handed pitchers (p = 0.000577).

* throwsS: Switch-handed pitchers show no significant difference compared to left-handed pitchers (p = 0.943235), likely due to sparse data.

Goodness-of-Fit

1. Null Deviance: 26122, representing the deviance of the intercept-only model.

2. Residual Deviance: 10607, indicating that the predictors explain a substantial amount of variability in the number of shutouts.

3. AIC: 18025, which can be used for model comparison with alternative models.

Analysis of Deviance Table

The sequential addition of predictors shows their individual contributions to reducing the deviance:

1. Innings

Explains the largest proportion of deviance, reducing it by 15359.9 (p < 2.2e-16), making it the most significant predictor.

2. Weight

Further reduces deviance by 85.6 (p < 2.2e-16), indicating a small but significant effect.

3. Height

Reduces deviance by 57.5 (p = 3.42e-14), showing a significant contribution.

4. Throws

Reduces deviance by 11.9 (p = 0.002618), suggesting that throwing hand has a minor but significant impact.

Interpretation of P-Values

A p-value measures the probability of observing the given data (or something more extreme) under the null hypothesis that a predictor has no effect. For example:

* A p < 0.05 indicates that we can reject the null hypothesis and conclude the predictor has a statistically significant effect.

* For innings, p < 2e-16 implies it is a highly significant predictor. Each additional inning contributes meaningfully to explaining shutouts.

* The lack of significance for throwsS (p = 0.943235) suggests no discernible difference between switch-handed and left-handed pitchers.

Conclusions

1. Most Important Predictor: Innings is the strongest predictor of shutouts, reflecting its practical and statistical significance.

2. Body Characteristics: Both weight (negative) and height (positive) significantly influence the expected number of shutouts.

3. Throws: Right-handed pitchers tend to achieve fewer shutouts compared to left-handed pitchers, while switch-handed pitchers show no significant difference.

4. Model Fit: The significant reduction in deviance and low residual deviance indicate that the model adequately explains the variation in shutouts, although additional variables might improve the fit further.


```{r}
# 3.e . Now create a new model, poisson_mod2, in which you also include teamID as a random effect. Ensure the code generates no warnings. Write out the form of the fitted model (rounded to 2 significant figures). From the model results, does teamID seem to be an important predictor? Why or why not? 

# Fit the mixed-effects Poisson model with teamID as a random effect

control <- glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))

poisson_mod2 <- glmer(SHO ~ innings + weight + height + throws + (1 | teamID), 
                      family = poisson, data = df_pitchers, control = control)

# Summarize the model

summary(poisson_mod2)

# Extract the random effects variance

ranef_variance <- as.data.frame(VarCorr(poisson_mod2))
print(ranef_variance)

# Write out the fitted model equation (rounded to 2 significant figures)

fixed_effects <- fixef(poisson_mod2)
fixed_effects_rounded <- round(fixed_effects, 2)
cat("Fitted model equation:\n")
cat("log(SHO) = ", 
    fixed_effects_rounded[1], " + ",
    fixed_effects_rounded["innings"], " * innings + ",
    fixed_effects_rounded["weight"], " * weight + ",
    fixed_effects_rounded["height"], " * height + ",
    fixed_effects_rounded["throwsL"], " * throwsL\n")

# Assess importance of teamID

team_variance <- ranef_variance$vcov[ranef_variance$grp == "teamID"]
cat("\nVariance of random effect (teamID):", round(team_variance, 2), "\n")

# Interpretation

if (team_variance > 0.1) {
  cat("TeamID appears to be an important predictor as it accounts for a significant amount of variance.\n")
} else {
  cat("TeamID does not appear to be an important predictor as its variance is minimal.\n")
}
```
Form of the fitted model: 

log(SHO)=−7.12+0.02⋅innings−0.01⋅weight+0.06⋅height+NA⋅throwsL

This equation highlights the effects of the predictors on the log-transformed count of shutouts. For example, an additional inning pitched increases the log count of shutouts by 0.02, while an additional unit of weight reduces it by 0.01.

Evaluation of teamID:

The model includes teamID as a random effect to account for variability across teams. The variance associated with teamID is $\sigma^2 = 0.0488$
, with a standard deviation of $\sigma^2 = 0.2209$. These values are extremely small, indicating that differences between teams explain very little of the variation in shutouts.

Why teamID is not important:

1. Low Variance Contribution: A variance of 0.0488 implies that teamID contributes less than 5% of the unexplained variation in SHO. This suggests that the number of shutouts is driven primarily by fixed effects (e.g., innings pitched, weight, height) rather than team-level differences.

2. Minimal Impact on Predictions: Given the low variability, including teamID in the model does not substantially improve model fit or predictive accuracy. Its inclusion may increase model complexity unnecessarily.

3. Biological Plausibility: The minimal effect aligns with the expectation that individual player attributes (innings, weight, height) have a much stronger influence on shutouts than team-level characteristics.

Addressing Convergence Warnings:

The model reports convergence issues, specifically a large eigenvalue ratio and a gradient close to the tolerance threshold. This suggests near-collinearity or poorly scaled predictors. These issues can affect the reliability of variance estimates for random effects, including teamID.

Rescaling Variables:

Rescaling predictors (e.g., centering or standardizing innings, weight, and height) may resolve convergence warnings. However, the small variance of teamID suggests that even with rescaling, its contribution would remain negligible.

Conclusion:

The random effect for teamID has a minimal impact on the model, as evidenced by its small variance and standard deviation. While including teamID accounts for team-level clustering, its negligible contribution suggests it is not an important predictor of shutouts. This conclusion is consistent with both statistical evidence (low variance) and theoretical expectations (individual performance dominates team effects).

Additional Considerations:

1. Comparing Models: For further analysis, compare the current mixed-effects model with a fixed-effects-only model (no teamID). If the AIC or residual deviance is similar, it reinforces the conclusion that teamID is unnecessary.

2. Practical Implications: In practice, the findings suggest that teams themselves have little influence on shutouts, emphasizing the importance of individual-level predictors in player performance evaluation.



```{r}
# 3.f Produce a scale-location plot for poisson_mod1. Explain what you observe and comment on any unusual pattern. 

# Scale-location plot

plot(poisson_mod1, which = 3)
```

This is a Scale-Location Plot, also known as a Spread-Location plot. It is used to check the homoscedasticity (constant variance) of residuals in a model. Here's the interpretation:

Observations:

1. General Pattern:

* The red line (smoothed trend line) appears to have a slight curvature. This indicates that the variance of residuals might not be constant, violating the assumption of homoscedasticity.

2. Spread of Residuals:

* Residuals (points) are not uniformly scattered around the red line. Instead, there appears to be a fan-shaped pattern:

* Residual variance increases as predicted values decrease. This is especially noticeable for larger negative predicted values.
Clusters and Outliers:

* Several clusters of points are present, suggesting that the residuals are grouped around specific predicted values. This might indicate some structure in the data that is not captured by the model.

* Some extreme values (e.g., observations labeled 14210, 22698, and 38432) deviate significantly from the general trend. These could represent outliers or influential observations that impact the model fit.

Comments on Unusual Patterns:

Heteroscedasticity:

The increase in variance with smaller predicted values (fan shape) suggests heteroscedasticity. This violates the assumption of constant variance and could lead to unreliable standard errors, p-values, and confidence intervals.

Possible Model Misfit:

The structure in the residuals could indicate that the model is not fully capturing the relationship between predictors and the response variable. This might be addressed by:
Adding interaction terms.
Applying transformations to variables.
Considering a different modeling approach, such as a generalized linear mixed model (GLMM) if random effects are present.

Outliers:

The labeled points are potential outliers or high-leverage observations. These points could disproportionately influence the model and may require further investigation to assess their validity and impact.

Recommendations:

1. Perform a Breusch-Pagan test or use robust standard errors to formally assess heteroscedasticity.

2. Investigate outliers to determine their cause and consider removing or adjusting the model if they are unduly influential.

3. Explore variable transformations or alternative model structures to better fit the data and address non-constant variance.


```{r}
# 3.g Using poisson_mod1, how many times more shutouts do left handed pitchers pitch, on average, than right handed pitchers. All other factors being equal, do taller or shorter players pitch more shutouts? Heavier or lighter? Explain why. 

# Determine the reference Category for throws

levels(df_pitchers$throws) # B, L, R, S


# Left vs. Right-handed pitchers effect

# Effect of right-handed pitchers compared to left-handed pitchers

exp(coef(poisson_mod1)["throwsR"])

# Effect of Left-handed pitchers compared to left-handed pitchers

exp(coef(poisson_mod1)["throwsL"])

# Effect of switch-handed pitchers compared to left-handed pitchers

exp(coef(poisson_mod1)["throwsS"])


# Taller vs. shorter and heavier vs. lighter players
# Interpret coefficients for height and weight

exp(coef(poisson_mod1)["height"])
exp(coef(poisson_mod1)["weight"])

# Overview of df_pitchers
# glimpse(df_pitchers)
# head(df_pitchers)
# tail(df_pitchers)
# summary(df_pitchers)

```











